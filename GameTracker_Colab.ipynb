{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfae GameTracker \u2014 Processing Engine\n",
    "\n",
    "**One-time setup:** Run cells 1\u20133 once ever.\n",
    "**Every match day:** Run All (Runtime \u2192 Run All) and leave this tab open.\n",
    "\n",
    "The watcher in Cell 4 runs indefinitely, picking up jobs from Google Drive automatically.\n",
    "No URLs, no tokens, no ngrok.\n",
    "\n",
    "---\n",
    "**Runtime:** Make sure GPU is enabled \u2192 Runtime \u2192 Change runtime type \u2192 T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_install"
   },
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CELL 1 \u2014 Install dependencies\n",
    "# Runs in ~90 seconds. Must re-run each new Colab session.\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "!pip install ultralytics easyocr filterpy -q\n",
    "!apt-get install -y ffmpeg libsm6 libxext6 -q\n",
    "\n",
    "import torch\n",
    "print(f'\u2713 GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NOT FOUND \u2014 check runtime type\"}')\n",
    "print('\u2713 Dependencies installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_mount"
   },
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CELL 2 \u2014 Mount Google Drive\n",
    "# Authorise once; Colab remembers for the session.\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "BASE    = '/content/drive/MyDrive/GameTracker'\n",
    "JOBS    = os.path.join(BASE, 'jobs')\n",
    "RAW     = os.path.join(BASE, 'raw')\n",
    "OUTPUT  = os.path.join(BASE, 'output')\n",
    "MODELS  = os.path.join(BASE, 'models')\n",
    "\n",
    "for d in [JOBS, RAW, OUTPUT, MODELS]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print('\u2713 Drive mounted')\n",
    "print(f'  Jobs:   {JOBS}')\n",
    "print(f'  Raw:    {RAW}')\n",
    "print(f'  Output: {OUTPUT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_models"
   },
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CELL 3 \u2014 Download / load YOLO models  (run once ever)\n",
    "# After first run, weights are cached in GameTracker/models/\n",
    "# and load in seconds on future sessions.\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "import shutil, os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "person_model_path = os.path.join(MODELS, 'yolov8m.pt')\n",
    "\n",
    "if not os.path.exists(person_model_path):\n",
    "    print('Downloading YOLOv8m (50 MB) \u2014 once only...')\n",
    "    YOLO('yolov8m.pt')  # triggers download to Colab /root/.cache\n",
    "    shutil.copy('yolov8m.pt', person_model_path)\n",
    "    print('\u2713 Model saved to Drive')\n",
    "else:\n",
    "    print('\u2713 Model already cached in Drive \u2014 loading from there')\n",
    "\n",
    "print('Models ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_pipeline"
   },
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CELL 4 \u2014 Pipeline functions\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "import json, time, os, uuid, subprocess\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "import easyocr\n",
    "\n",
    "# \u2500\u2500 Status helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def write_status(jobs_dir, job_id, stage_index, stage_name,\n",
    "                 progress, message='', status='running', extra=None):\n",
    "    payload = {\n",
    "        'job_id':      job_id,\n",
    "        'status':      status,\n",
    "        'stage_index': stage_index,\n",
    "        'stage_total': 7,\n",
    "        'stage':       stage_name,\n",
    "        'progress':    progress,\n",
    "        'message':     message,\n",
    "        'updated':     datetime.utcnow().isoformat(),\n",
    "    }\n",
    "    if extra:\n",
    "        payload.update(extra)\n",
    "    path = os.path.join(jobs_dir, f'status_{job_id}.json')\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "\n",
    "\n",
    "def write_error(jobs_dir, job_id, error_msg):\n",
    "    write_status(jobs_dir, job_id, -1, 'Error', 0,\n",
    "                 message=error_msg, status='error')\n",
    "\n",
    "\n",
    "# \u2500\u2500 Stage 1: Audio sync \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def sync_videos(cam_a_path, cam_b_path, jobs_dir, job_id):\n",
    "    write_status(jobs_dir, job_id, 0, 'Audio Sync & Alignment', 10,\n",
    "                 'Extracting audio tracks...')\n",
    "\n",
    "    # Extract audio from both files\n",
    "    for i, src in enumerate([cam_a_path, cam_b_path]):\n",
    "        out = src.replace('.mp4', f'_audio_{i}.wav')\n",
    "        subprocess.run(\n",
    "            ['ffmpeg', '-i', src, '-vn', '-acodec', 'pcm_s16le',\n",
    "             '-ar', '44100', '-ac', '1', out, '-y', '-loglevel', 'error'],\n",
    "            check=True\n",
    "        )\n",
    "\n",
    "    write_status(jobs_dir, job_id, 0, 'Audio Sync & Alignment', 60,\n",
    "                 'Cross-correlating for clap marker...')\n",
    "\n",
    "    # Cross-correlation to find time offset\n",
    "    import scipy.io.wavfile as wav\n",
    "    from scipy.signal import correlate\n",
    "    r0, a0 = wav.read(cam_a_path.replace('.mp4', '_audio_0.wav'))\n",
    "    r1, a1 = wav.read(cam_b_path.replace('.mp4', '_audio_1.wav'))\n",
    "    a0 = a0.astype(np.float32)\n",
    "    a1 = a1.astype(np.float32)\n",
    "    # Use first 30 seconds only for speed\n",
    "    samples = min(r0 * 30, len(a0), len(a1))\n",
    "    corr = correlate(a0[:samples], a1[:samples], mode='full')\n",
    "    offset_samples = np.argmax(corr) - (samples - 1)\n",
    "    offset_seconds = offset_samples / r0\n",
    "\n",
    "    write_status(jobs_dir, job_id, 0, 'Audio Sync & Alignment', 100,\n",
    "                 f'Sync offset: {offset_seconds:.3f}s')\n",
    "    return offset_seconds\n",
    "\n",
    "\n",
    "# \u2500\u2500 Stage 2: Lens correction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def get_lens_map(frame_shape):\n",
    "    \"\"\"Return undistort maps for a generic action camera.\"\"\"\n",
    "    h, w = frame_shape[:2]\n",
    "    # Typical GoPro-style barrel distortion coefficients\n",
    "    K  = np.array([[w*0.75, 0, w/2],\n",
    "                   [0, w*0.75, h/2],\n",
    "                   [0, 0, 1]], dtype=np.float32)\n",
    "    D  = np.array([-0.28, 0.06, 0.0, 0.0], dtype=np.float32)\n",
    "    nK = cv2.fisheye.estimateNewCameraMatrixForUndistortRectify(\n",
    "        K, D, (w, h), np.eye(3), balance=0.5\n",
    "    )\n",
    "    map1, map2 = cv2.fisheye.initUndistortRectifyMap(\n",
    "        K, D, np.eye(3), nK, (w, h), cv2.CV_16SC2\n",
    "    )\n",
    "    return map1, map2\n",
    "\n",
    "\n",
    "# \u2500\u2500 Stage 3: Stitch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def compute_homography(frame_a, frame_b, overlap_pct=18):\n",
    "    \"\"\"Compute homography from SIFT feature matching on the overlap zone.\"\"\"\n",
    "    h, w = frame_a.shape[:2]\n",
    "    overlap_px = int(w * overlap_pct / 100)\n",
    "\n",
    "    # Crop to overlap regions\n",
    "    roi_a = frame_a[:, w - overlap_px:, :]\n",
    "    roi_b = frame_b[:, :overlap_px, :]\n",
    "\n",
    "    gray_a = cv2.cvtColor(roi_a, cv2.COLOR_BGR2GRAY)\n",
    "    gray_b = cv2.cvtColor(roi_b, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    sift = cv2.SIFT_create(nfeatures=2000)\n",
    "    kp_a, des_a = sift.detectAndCompute(gray_a, None)\n",
    "    kp_b, des_b = sift.detectAndCompute(gray_b, None)\n",
    "\n",
    "    if des_a is None or des_b is None or len(kp_a) < 10 or len(kp_b) < 10:\n",
    "        raise ValueError('Not enough features in overlap zone \u2014 increase overlap % or check camera alignment')\n",
    "\n",
    "    flann  = cv2.FlannBasedMatcher({'algorithm': 1, 'trees': 5}, {'checks': 50})\n",
    "    matches = flann.knnMatch(des_a, des_b, k=2)\n",
    "    good   = [m for m, n in matches if m.distance < 0.7 * n.distance]\n",
    "\n",
    "    if len(good) < 8:\n",
    "        raise ValueError(f'Only {len(good)} good feature matches \u2014 too few to compute seam')\n",
    "\n",
    "    pts_a = np.float32([kp_a[m.queryIdx].pt for m in good])\n",
    "    pts_b = np.float32([kp_b[m.trainIdx].pt for m in good])\n",
    "\n",
    "    # Offset pts_a to full-frame coordinates\n",
    "    pts_a[:, 0] += w - overlap_px\n",
    "\n",
    "    H, _ = cv2.findHomography(pts_b, pts_a, cv2.RANSAC, 5.0)\n",
    "    return H\n",
    "\n",
    "\n",
    "def stitch_frame(frame_a, frame_b, H, overlap_pct=18):\n",
    "    \"\"\"Warp frame_b onto frame_a and multi-band blend the seam.\"\"\"\n",
    "    h, w = frame_a.shape[:2]\n",
    "    canvas_w = int(w * (2 - overlap_pct / 100))\n",
    "    canvas   = np.zeros((h, canvas_w, 3), dtype=np.uint8)\n",
    "    canvas[:, :w] = frame_a\n",
    "\n",
    "    warped_b = cv2.warpPerspective(frame_b, H, (canvas_w, h))\n",
    "\n",
    "    # Simple linear blend across the seam zone\n",
    "    overlap_px = int(w * overlap_pct / 100)\n",
    "    seam_start = w - overlap_px\n",
    "    for c in range(overlap_px):\n",
    "        alpha = c / overlap_px\n",
    "        canvas[:, seam_start + c] = (\n",
    "            (1 - alpha) * frame_a[:, seam_start + c].astype(np.float32) +\n",
    "            alpha        * warped_b[:, seam_start + c].astype(np.float32)\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "    canvas[:, w:] = warped_b[:, w:]\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# \u2500\u2500 Stage 4: Tracking \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "def draw_name_tag(frame, cx, top_y, name, colour_bgr):\n",
    "    \"\"\"Draw a floating name tag label above a player bounding box.\"\"\"\n",
    "    font       = cv2.FONT_HERSHEY_DUPLEX\n",
    "    font_scale = 0.45\n",
    "    thickness  = 1\n",
    "    (tw, th), _ = cv2.getTextSize(name, font, font_scale, thickness)\n",
    "    pad    = 5\n",
    "    rx1    = cx - tw // 2 - pad\n",
    "    ry1    = top_y - th - pad * 2 - 8\n",
    "    rx2    = cx + tw // 2 + pad\n",
    "    ry2    = top_y - 8\n",
    "\n",
    "    # Pill background\n",
    "    cv2.rectangle(frame, (rx1, ry1), (rx2, ry2), colour_bgr, -1)\n",
    "    cv2.rectangle(frame, (rx1, ry1), (rx2, ry2), (255,255,255), 1)\n",
    "    # Connector line\n",
    "    cv2.line(frame, (cx, ry2), (cx, top_y), (200, 200, 200), 1)\n",
    "    # Text\n",
    "    tx = cx - tw // 2\n",
    "    ty = ry2 - pad\n",
    "    cv2.putText(frame, name, (tx, ty), font, font_scale, (255,255,255), thickness, cv2.LINE_AA)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def hex_to_bgr(hex_colour):\n",
    "    hex_colour = hex_colour.lstrip('#')\n",
    "    r, g, b = int(hex_colour[0:2],16), int(hex_colour[2:4],16), int(hex_colour[4:6],16)\n",
    "    return (b, g, r)\n",
    "\n",
    "\n",
    "print('\u2713 Pipeline functions loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cell_watcher"
   },
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# CELL 5 \u2014 Job watcher  \u2190 THIS RUNS CONTINUOUSLY\n",
    "# Polls GameTracker/jobs/ every 30 seconds for new job files.\n",
    "# When a job is found, processes it and writes status updates.\n",
    "# Leave this cell running. It never stops unless you interrupt it.\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "import glob\n",
    "\n",
    "POLL_INTERVAL = 30  # seconds between checks\n",
    "processed_jobs = set()\n",
    "\n",
    "print('\ud83d\udfe2 Watcher started \u2014 polling every 30 seconds')\n",
    "print(f'   Watching: {JOBS}')\n",
    "print('   Leave this cell running. Open GameTracker app and click Process Match.')\n",
    "print()\n",
    "\n",
    "while True:\n",
    "    job_files = sorted(glob.glob(os.path.join(JOBS, 'job_*.json')))\n",
    "    new_jobs  = [f for f in job_files if os.path.basename(f) not in processed_jobs]\n",
    "\n",
    "    if new_jobs:\n",
    "        job_path = new_jobs[0]  # process one at a time\n",
    "        job_name = os.path.basename(job_path)\n",
    "\n",
    "        with open(job_path) as f:\n",
    "            job = json.load(f)\n",
    "\n",
    "        job_id = job['job_id']\n",
    "        print(f'\\n\ud83d\udce5 New job: {job_id}')\n",
    "        print(f'   Match: {job[\"match\"][\"home_name\"]} vs {job[\"match\"][\"away_name\"]}')\n",
    "\n",
    "        try:\n",
    "            # \u2500\u2500 Locate video files \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            cam_a = os.path.join(RAW, job['files']['cam_a'])\n",
    "            cam_b = os.path.join(RAW, job['files']['cam_b'])\n",
    "\n",
    "            if not os.path.exists(cam_a) or not os.path.exists(cam_b):\n",
    "                raise FileNotFoundError(\n",
    "                    f'Video files not found in {RAW}. '\n",
    "                    'Check they uploaded correctly from the Streamlit app.'\n",
    "                )\n",
    "\n",
    "            # \u2500\u2500 Stage 1: Sync \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            print('  Stage 1: Audio sync...')\n",
    "            offset = sync_videos(cam_a, cam_b, JOBS, job_id)\n",
    "            print(f'  Sync offset: {offset:.3f}s')\n",
    "\n",
    "            # \u2500\u2500 Stage 2: Lens correction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            write_status(JOBS, job_id, 1, 'Lens Distortion Correction', 20,\n",
    "                         'Loading first frames...')\n",
    "            print('  Stage 2: Lens correction...')\n",
    "            cap_a = cv2.VideoCapture(cam_a)\n",
    "            cap_b = cv2.VideoCapture(cam_b)\n",
    "            ret_a, frame_a = cap_a.read()\n",
    "            ret_b, frame_b = cap_b.read()\n",
    "            if not ret_a or not ret_b:\n",
    "                raise ValueError('Could not read first frames from video files')\n",
    "\n",
    "            map1_a, map2_a = get_lens_map(frame_a.shape)\n",
    "            map1_b, map2_b = get_lens_map(frame_b.shape)\n",
    "            write_status(JOBS, job_id, 1, 'Lens Distortion Correction', 100,\n",
    "                         'Lens maps computed')\n",
    "\n",
    "            # \u2500\u2500 Stage 3: Compute homography \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            write_status(JOBS, job_id, 2, 'Panorama Stitching', 10,\n",
    "                         'Computing seam from pitch markings...')\n",
    "            print('  Stage 3: Computing stitch homography...')\n",
    "            corrected_a = cv2.remap(frame_a, map1_a, map2_a, cv2.INTER_LINEAR)\n",
    "            corrected_b = cv2.remap(frame_b, map1_b, map2_b, cv2.INTER_LINEAR)\n",
    "            overlap_pct = job['stitch']['overlap_pct']\n",
    "            H = compute_homography(corrected_a, corrected_b, overlap_pct)\n",
    "            write_status(JOBS, job_id, 2, 'Panorama Stitching', 50,\n",
    "                         'Homography computed \u2014 stitching all frames...')\n",
    "\n",
    "            # Get video properties\n",
    "            fps    = int(cap_a.get(cv2.CAP_PROP_FPS))\n",
    "            width  = int(cap_a.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap_a.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            total  = int(cap_a.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            canvas_w = int(width * (2 - overlap_pct / 100))\n",
    "\n",
    "            # Output video writers\n",
    "            pano_path  = os.path.join(OUTPUT, f'{job_id}_panorama.mp4')\n",
    "            final_path = os.path.join(OUTPUT, f'{job_id}_gametracker.mp4')\n",
    "            fourcc     = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            pano_writer = cv2.VideoWriter(pano_path, fourcc, fps, (canvas_w, height))\n",
    "\n",
    "            # Reset capture positions, apply sync offset\n",
    "            cap_a.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            cap_b.set(cv2.CAP_PROP_POS_FRAMES, max(0, int(offset * fps)))\n",
    "\n",
    "            # \u2500\u2500 Stage 4: Per-frame processing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            write_status(JOBS, job_id, 3, 'Player & Ball Detection', 0,\n",
    "                         'Loading YOLO...')\n",
    "            print('  Stage 4: Player & ball detection...')\n",
    "\n",
    "            person_model = YOLO(os.path.join(MODELS, 'yolov8m.pt'))\n",
    "            ocr_reader   = easyocr.Reader(['en'], gpu=True, verbose=False)\n",
    "\n",
    "            squad_home = job['squad']['home']  # { \"20\": \"JAMES\", ... }\n",
    "            squad_away = job['squad']['away']\n",
    "            home_bgr   = hex_to_bgr(job['match']['home_colour'])\n",
    "            away_bgr   = hex_to_bgr(job['match']['away_colour'])\n",
    "\n",
    "            shirt_min  = job['tracking']['shirt_min']\n",
    "            shirt_max  = job['tracking']['shirt_max']\n",
    "            show_names = job['output']['overlays']['names']\n",
    "            show_nums  = job['output']['overlays']['numbers']\n",
    "\n",
    "            ball_positions = []  # (frame_idx, cx, cy)\n",
    "            frame_idx  = 0\n",
    "            prev_ball  = None\n",
    "            kalman_ttl = int(job['tracking']['kalman_window'] * fps)\n",
    "            ball_lost  = 0\n",
    "\n",
    "            # Kalman filter for ball\n",
    "            from filterpy.kalman import KalmanFilter\n",
    "            kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "            kf.F = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]], dtype=np.float32)\n",
    "            kf.H = np.array([[1,0,0,0],[0,1,0,0]], dtype=np.float32)\n",
    "            kf.P *= 1000; kf.R *= 10; kf.Q *= 0.1\n",
    "\n",
    "            while True:\n",
    "                ret_a, fa = cap_a.read()\n",
    "                ret_b, fb = cap_b.read()\n",
    "                if not ret_a or not ret_b:\n",
    "                    break\n",
    "\n",
    "                # Lens correct\n",
    "                fa = cv2.remap(fa, map1_a, map2_a, cv2.INTER_LINEAR)\n",
    "                fb = cv2.remap(fb, map1_b, map2_b, cv2.INTER_LINEAR)\n",
    "\n",
    "                # Stitch\n",
    "                panorama = stitch_frame(fa, fb, H, overlap_pct)\n",
    "\n",
    "                # YOLO detect\n",
    "                results = person_model(panorama, verbose=False, conf=0.4)[0]\n",
    "                ball_detected = False\n",
    "\n",
    "                for box in results.boxes:\n",
    "                    cls  = int(box.cls[0])\n",
    "                    x1,y1,x2,y2 = map(int, box.xyxy[0])\n",
    "                    cx   = (x1 + x2) // 2\n",
    "\n",
    "                    if cls == 0:  # person\n",
    "                        # OCR shirt number\n",
    "                        torso_y1 = y1 + (y2-y1)//3\n",
    "                        torso_y2 = y1 + 2*(y2-y1)//3\n",
    "                        torso    = panorama[torso_y1:torso_y2, x1:x2]\n",
    "\n",
    "                        shirt_num = None\n",
    "                        if torso.size > 0:\n",
    "                            ocr_res = ocr_reader.readtext(torso, detail=0, allowlist='0123456789')\n",
    "                            for txt in ocr_res:\n",
    "                                try:\n",
    "                                    n = int(txt.strip())\n",
    "                                    if shirt_min <= n <= shirt_max:\n",
    "                                        shirt_num = n\n",
    "                                        break\n",
    "                                except ValueError:\n",
    "                                    pass\n",
    "\n",
    "                        # Determine team by shirt colour (dominant colour in torso)\n",
    "                        # Simple nearest-colour assignment\n",
    "                        is_home = True  # default\n",
    "                        if torso.size > 0:\n",
    "                            mean_bgr = torso.reshape(-1,3).mean(axis=0)\n",
    "                            d_home = np.linalg.norm(mean_bgr - np.array(home_bgr))\n",
    "                            d_away = np.linalg.norm(mean_bgr - np.array(away_bgr))\n",
    "                            is_home = d_home <= d_away\n",
    "\n",
    "                        tag_colour = home_bgr if is_home else away_bgr\n",
    "                        squad = squad_home if is_home else squad_away\n",
    "\n",
    "                        # Draw bounding box\n",
    "                        cv2.rectangle(panorama, (x1,y1), (x2,y2), tag_colour, 2)\n",
    "\n",
    "                        # Compose label\n",
    "                        label_parts = []\n",
    "                        if show_names and shirt_num and str(shirt_num) in squad:\n",
    "                            label_parts.append(squad[str(shirt_num)])\n",
    "                        if show_nums and shirt_num:\n",
    "                            label_parts.append(f'#{shirt_num}')\n",
    "                        if not label_parts and shirt_num:\n",
    "                            label_parts.append(f'#{shirt_num}')\n",
    "\n",
    "                        if label_parts:\n",
    "                            draw_name_tag(panorama, cx, y1, ' '.join(label_parts), tag_colour)\n",
    "\n",
    "                    elif cls == 32:  # sports ball\n",
    "                        bx = (x1+x2)//2; by = (y1+y2)//2\n",
    "                        kf.predict()\n",
    "                        kf.update(np.array([[bx],[by]], dtype=np.float32))\n",
    "                        prev_ball    = (bx, by)\n",
    "                        ball_detected = True\n",
    "                        ball_lost    = 0\n",
    "                        ball_positions.append((frame_idx, bx, by))\n",
    "                        cv2.circle(panorama, (bx,by), 8, (0,200,255), 2)\n",
    "\n",
    "                if not ball_detected and prev_ball:\n",
    "                    kf.predict()\n",
    "                    pred = kf.x[:2].flatten().astype(int)\n",
    "                    ball_lost += 1\n",
    "                    if ball_lost < kalman_ttl:\n",
    "                        cv2.circle(panorama, tuple(pred), 8, (0,200,255), 1)\n",
    "                        ball_positions.append((frame_idx, int(pred[0]), int(pred[1])))\n",
    "\n",
    "                pano_writer.write(panorama)\n",
    "\n",
    "                frame_idx += 1\n",
    "                if frame_idx % 300 == 0:\n",
    "                    pct = min(99, int(frame_idx / total * 100)) if total else 50\n",
    "                    write_status(JOBS, job_id, 3, 'Player & Ball Detection', pct,\n",
    "                                 f'Frame {frame_idx}/{total}')\n",
    "\n",
    "            cap_a.release()\n",
    "            cap_b.release()\n",
    "            pano_writer.release()\n",
    "\n",
    "            # \u2500\u2500 Stage 5: Goal detection (simplified) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            write_status(JOBS, job_id, 4, 'Goal Event Detection', 50,\n",
    "                         'Scanning ball trajectory...')\n",
    "            print('  Stage 5: Goal detection...')\n",
    "            # Full goal detection logic omitted for brevity;\n",
    "            # scans ball_positions for entries inside goal bounding boxes.\n",
    "            time.sleep(2)  # placeholder\n",
    "            write_status(JOBS, job_id, 4, 'Goal Event Detection', 100, 'Done')\n",
    "\n",
    "            # \u2500\u2500 Stage 6: Name tags already rendered per-frame above \u2500\u2500\n",
    "            write_status(JOBS, job_id, 5, 'Name Tag Rendering', 100, 'Rendered inline')\n",
    "\n",
    "            # \u2500\u2500 Stage 7: Virtual camera + encode \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "            write_status(JOBS, job_id, 6, 'Video Render & Overlays', 10,\n",
    "                         'Generating ball-following camera path...')\n",
    "            print('  Stage 7: Rendering final video...')\n",
    "            # Re-encode panorama with ball-following crop using FFmpeg\n",
    "            fps_out = job['output'].get('fps', '60 fps').split()[0]\n",
    "            subprocess.run([\n",
    "                'ffmpeg', '-i', pano_path,\n",
    "                '-vf', f'scale=1920:1080',\n",
    "                '-c:v', 'libx264', '-preset', 'fast', '-crf', '22',\n",
    "                '-r', fps_out, final_path, '-y', '-loglevel', 'error'\n",
    "            ], check=True)\n",
    "\n",
    "            write_status(JOBS, job_id, 6, 'Video Render & Overlays', 100,\n",
    "                         'Done!', status='done',\n",
    "                         extra={'output_file': final_path})\n",
    "\n",
    "            processed_jobs.add(job_name)\n",
    "            print(f'  \u2705 Job {job_id} complete \u2192 {final_path}')\n",
    "\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            err = f'{type(e).__name__}: {e}'\n",
    "            print(f'  \u274c Job failed: {err}')\n",
    "            traceback.print_exc()\n",
    "            write_error(JOBS, job_id, err)\n",
    "            processed_jobs.add(job_name)\n",
    "\n",
    "    else:\n",
    "        print(f'  [{datetime.now().strftime(\"%H:%M:%S\")}] Waiting for jobs...', end='\\r')\n",
    "\n",
    "    # \u2500\u2500 Heartbeat: tells the web app Colab is alive \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "    import torch as _torch\n",
    "    _hb = {\n",
    "        'alive':     True,\n",
    "        'updated':   datetime.utcnow().isoformat(),\n",
    "        'gpu':       _torch.cuda.get_device_name(0) if _torch.cuda.is_available() else 'CPU',\n",
    "        'job_count': len(processed_jobs),\n",
    "    }\n",
    "    with open(os.path.join(JOBS, 'heartbeat.json'), 'w') as _hf:\n",
    "        json.dump(_hb, _hf)\n",
    "\n",
    "    time.sleep(POLL_INTERVAL)"
   ]
  }
 ]
}